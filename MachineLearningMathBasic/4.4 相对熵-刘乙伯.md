4.4相对熵** **熵** 熵在物理理中表示的是一个系统的混乱程度

 **信息熵** 表示的是某个概率分布的不确定程度
 e g. 当⼀个概率分布的概率为99%则她的不确定性就越小，相应的信息熵越小 当这个事件发⽣生的概率为50%时她的不确定性就大，则相应的信息熵就更大 

信息熵的值域(0，1) 

公式 

![page1image34935056.jpg](/Users/liuyibo/Library/Application Support/typora-user-images/page1image34935056-6977876.jpg) 

![page2image34769136.jpg](/Users/liuyibo/Library/Application Support/typora-user-images/page2image34769136-6977876.jpg)

其中x表示随机变量量可能的取值，与p(x)表示的是x发⽣生的概率 b是底数一般⽤用2和e 

eg. 

![page2image34766848.jpg](/Users/liuyibo/Library/Application Support/typora-user-images/page2image34766848-6977876.jpg) ![page2image34767056.jpg](/Users/liuyibo/Library/Application Support/typora-user-images/page2image34767056-6977876.jpg)

**相对熵** 

相对熵是啥 

就是如果有两个单独的概率分布P(X)和P(X)，我们⽤用相对熵来衡量量这两个分布的差异 

公式 

![page3image35059104.jpg](/Users/liuyibo/Library/Application Support/typora-user-images/page3image35059104-6977876.jpg)

在机器学习分类任务中，P往往用来表示样本的真实分布，比如[1,0,0]表示当前样本属于第一类。Q⽤来 表示模型所预测的分布，⽐比如[0.7,0.2,0.1] 。 

性质
 (1)kl(P||Q) >= 0，无最大值 (2)不对称 KL(P||Q) != KL(Q||P) (3)不满⾜三⻆不等式 

(4)DKL的值越⼩，表示q分布和p分布越接近

 (2)的计算过程 

![page3image35052032.jpg](/Users/liuyibo/Library/Application Support/typora-user-images/page3image35052032-6977876.jpg) ![page3image35050160.jpg](/Users/liuyibo/Library/Application Support/typora-user-images/page3image35050160-6977876.jpg)

python中相对熵怎么搞：

import numpy as np

import scipy.stats

p=np.asarray([0.65,0.25,0.07,0.03])

q=np.array([0.6,0.25,0.1,0.05])

\#⽅法一:根据公式求解kl1=np.sum(p*np.log(p/q))print(kl1)

⽅法二:调⽤用scipy包求解 kl2=scipy.stats.entropy(p, q) print(kl2) 